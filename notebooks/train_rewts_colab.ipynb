{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ReWTS-LLM-RL Training su Google Colab\n",
    "\n",
    "Questo notebook permette di eseguire il training del sistema ReWTSE-LLM-RL su Google Colab.\n",
    "\n",
    "## Setup Necessario\n",
    "\n",
    "1. Carica il tuo progetto su Google Drive nella cartella `MyDrive/Papers`\n",
    "2. Configura la tua API key di Gemini nei Secrets di Colab (üîë)\n",
    "3. Assicurati di avere i dati preprocessati nella cartella `data/processed/`\n",
    "4. Abilita GPU: Runtime > Change runtime type > GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## 1. Verifica GPU disponibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Verifica se CUDA √® disponibile\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU disponibile: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö† GPU non disponibile. Il training sar√† pi√π lento.\")\n",
    "    print(\"  Per abilitare GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 2. Monta Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úì Google Drive montato con successo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 3. ‚öôÔ∏è CONFIGURAZIONE CENTRALIZZATA\n",
    "\n",
    "**IMPORTANTE:** Modifica questa cella per configurare il training.\n",
    "\n",
    "### Parametri da modificare:\n",
    "- `PROJECT_PATH`: Percorso al progetto su Google Drive\n",
    "- `tickers`: Lista dei ticker da processare\n",
    "- `chunk_length`: Lunghezza dei chunk temporali\n",
    "- `episodes_per_chunk`: Numero di episodi per chunk (riduci per training pi√π veloce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_all"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURAZIONE PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Path al progetto su Google Drive (MODIFICA SE NECESSARIO)\n",
    "PROJECT_PATH = '/content/drive/MyDrive/Papers'\n",
    "\n",
    "# Cambia working directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURAZIONE TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    # -----------------------------------------------------\n",
    "    # Tickers da processare\n",
    "    # -----------------------------------------------------\n",
    "    'tickers': ['AAPL'],  # Lista dei ticker (es: ['AAPL', 'MSFT', 'GOOGL'])\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Configurazione LLM (Google Gemini)\n",
    "    # -----------------------------------------------------\n",
    "    'llm': {\n",
    "        'llm_model': 'gemini-2.0-flash-exp',  # Modello Gemini da usare\n",
    "        'temperature': 0.0,                    # Temperature per generazione (0.0 = deterministico)\n",
    "        'seed': 49,                            # Seed per riproducibilit√†\n",
    "        'gemini_api_key': None                 # Verr√† impostato dalla cella API key\n",
    "    },\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Configurazione ReWTSE (Reinforcement Learning)\n",
    "    # -----------------------------------------------------\n",
    "    'rewts': {\n",
    "        'chunk_length': 500,           # Lunghezza di ogni chunk temporale (giorni di trading)\n",
    "        'lookback_length': 100,        # Finestra di lookback per le features\n",
    "        'forecast_horizon': 1,         # Orizzonte di previsione (giorni)\n",
    "        'episodes_per_chunk': 20,      # Episodi di training per chunk (riduci per velocit√†)\n",
    "        \n",
    "        # Parametri DDQN\n",
    "        'gamma': 0.99,                 # Discount factor (quanto valutare reward futuri)\n",
    "        'epsilon_start': 1.0,          # Epsilon iniziale per exploration\n",
    "        'epsilon_min': 0.01,           # Epsilon minimo\n",
    "        'epsilon_decay': 0.995,        # Decay rate di epsilon\n",
    "        'learning_rate': 1e-3,         # Learning rate per optimizer\n",
    "        'batch_size': 64,              # Batch size per training\n",
    "        'buffer_size': 10000,          # Dimensione del replay buffer\n",
    "        'target_update_freq': 10,      # Frequenza aggiornamento target network\n",
    "        'hidden_dims': [128, 64]       # Architettura rete neurale (hidden layers)\n",
    "    },\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Configurazione Trading Environment\n",
    "    # -----------------------------------------------------\n",
    "    'trading_env': {\n",
    "        'initial_balance': 10000,      # Capitale iniziale ($)\n",
    "        'transaction_cost': 0.001,     # Costo di transazione (0.1%)\n",
    "        'max_position': 1.0            # Posizione massima (frazione del capitale)\n",
    "    },\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Frequenza generazione strategie LLM\n",
    "    # -----------------------------------------------------\n",
    "    'strategy_frequency': 20  # Genera nuova strategia ogni N giorni di trading\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STAMPA CONFIGURAZIONE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURAZIONE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nProject Path: {PROJECT_PATH}\")\n",
    "print(f\"\\nTickers: {TRAINING_CONFIG['tickers']}\")\n",
    "print(f\"\\nLLM:\")\n",
    "print(f\"  - Model: {TRAINING_CONFIG['llm']['llm_model']}\")\n",
    "print(f\"  - Temperature: {TRAINING_CONFIG['llm']['temperature']}\")\n",
    "print(f\"  - Seed: {TRAINING_CONFIG['llm']['seed']}\")\n",
    "print(f\"\\nReWTSE:\")\n",
    "print(f\"  - Chunk length: {TRAINING_CONFIG['rewts']['chunk_length']} giorni\")\n",
    "print(f\"  - Episodes per chunk: {TRAINING_CONFIG['rewts']['episodes_per_chunk']}\")\n",
    "print(f\"  - Strategy frequency: ogni {TRAINING_CONFIG['strategy_frequency']} giorni\")\n",
    "print(f\"\\nTrading:\")\n",
    "print(f\"  - Initial balance: ${TRAINING_CONFIG['trading_env']['initial_balance']}\")\n",
    "print(f\"  - Transaction cost: {TRAINING_CONFIG['trading_env']['transaction_cost']*100}%\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "## 4. Installa dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": [
    "# Installa le dipendenze\n",
    "print(\"Installazione dipendenze...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verifica installazione\n",
    "import google.generativeai as genai\n",
    "print(\"‚úì Tutte le dipendenze installate correttamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api_key"
   },
   "source": [
    "## 5. Configura API Key\n",
    "\n",
    "### Setup Sicuro API Key (Raccomandato):\n",
    "\n",
    "1. Clicca sull'icona della chiave üîë nella barra laterale sinistra\n",
    "2. Aggiungi un nuovo secret: `GEMINI_API_KEY`\n",
    "3. Incolla la tua API key di Google Gemini\n",
    "4. Abilita \"Notebook access\"\n",
    "\n",
    "**Ottieni la tua API key gratuita:** https://ai.google.dev/\n",
    "\n",
    "### Metodo Alternativo (File .env):\n",
    "\n",
    "Se preferisci usare un file `.env` su Drive, decommenta il codice alternativo nella cella sotto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_setup"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 1: Colab Secrets (RACCOMANDATO)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    # Leggi la API key dai secrets di Colab\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
    "    \n",
    "    # Aggiorna config\n",
    "    TRAINING_CONFIG['llm']['gemini_api_key'] = GEMINI_API_KEY\n",
    "    \n",
    "    # Mostra key mascherata per verifica\n",
    "    masked_key = f\"{GEMINI_API_KEY[:10]}...{GEMINI_API_KEY[-4:]}\"\n",
    "    print(f\"‚úì API key configurata: {masked_key}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Errore Colab Secrets: {e}\")\n",
    "    print(\"\\nConfigura GEMINI_API_KEY nei secrets di Colab (icona üîë)\")\n",
    "    print(\"Oppure usa il metodo alternativo .env (vedi sotto)\")\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 2: File .env (ALTERNATIVO)\n",
    "# ============================================================================\n",
    "# Decommenta se preferisci usare un file .env su Google Drive\n",
    "\n",
    "# env_path = os.path.join(PROJECT_PATH, '.env')\n",
    "# if os.path.exists(env_path):\n",
    "#     with open(env_path) as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if line and not line.startswith('#') and '=' in line:\n",
    "#                 key, value = line.split('=', 1)\n",
    "#                 os.environ[key] = value\n",
    "#     \n",
    "#     GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "#     TRAINING_CONFIG['llm']['gemini_api_key'] = GEMINI_API_KEY\n",
    "#     print(f\"‚úì API key caricata da .env: {GEMINI_API_KEY[:10]}...{GEMINI_API_KEY[-4:]}\")\n",
    "# else:\n",
    "#     print(f\"‚ö† File .env non trovato in {env_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICA API KEY\n",
    "# ============================================================================\n",
    "\n",
    "if TRAINING_CONFIG['llm']['gemini_api_key']:\n",
    "    print(\"\\n‚úÖ Configurazione API completata - Puoi procedere\")\n",
    "else:\n",
    "    print(\"\\n‚ùå API key non configurata - Configura prima di procedere\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_data"
   },
   "source": [
    "## 6. Verifica dati disponibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Verifica che i dati siano presenti\n",
    "data_dir = 'data/processed'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICA DATI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    print(f\"\\n‚úì Trovati {len(files)} file CSV in {data_dir}:\")\n",
    "    \n",
    "    total_size = 0\n",
    "    for f in sorted(files):\n",
    "        file_path = os.path.join(data_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  - {f:40s} {size_mb:>8.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nTotale: {total_size:.2f} MB\")\n",
    "    \n",
    "    # Verifica che ci siano i file necessari per ogni ticker\n",
    "    print(\"\\nVerifica file per ticker:\")\n",
    "    all_ok = True\n",
    "    for ticker in TRAINING_CONFIG['tickers']:\n",
    "        market_file = f\"{ticker}_full_data.csv\"\n",
    "        news_file = f\"{ticker}_news.csv\"\n",
    "        \n",
    "        has_market = market_file in files\n",
    "        has_news = news_file in files\n",
    "        \n",
    "        status = \"‚úì\" if (has_market and has_news) else \"‚úó\"\n",
    "        print(f\"  {status} {ticker}: Market={has_market}, News={has_news}\")\n",
    "        \n",
    "        if not (has_market and has_news):\n",
    "            all_ok = False\n",
    "    \n",
    "    if all_ok:\n",
    "        print(\"\\n‚úÖ Tutti i dati necessari sono presenti\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† ATTENZIONE: Mancano alcuni file necessari\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ùå Directory {data_dir} non trovata!\")\n",
    "    print(\"\\nAssicurati di:\")\n",
    "    print(\"  1. Aver preprocessato i dati\")\n",
    "    print(\"  2. Aver caricato la cartella 'data/processed/' su Google Drive\")\n",
    "    print(f\"  3. Il PROJECT_PATH sia corretto: {PROJECT_PATH}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_modules"
   },
   "source": [
    "## 7. Importa moduli del progetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Aggiungi il progetto al path\n",
    "if PROJECT_PATH not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_PATH)\n",
    "\n",
    "# Importa i moduli del progetto\n",
    "from src.llm_agents.strategist_agent import StrategistAgent\n",
    "from src.llm_agents.analyst_agent import AnalystAgent\n",
    "from src.rl_agents.trading_env import TradingEnv\n",
    "from src.hybrid_model.ensemble_controller import ReWTSEnsembleController\n",
    "\n",
    "print(\"‚úì Moduli importati correttamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_functions"
   },
   "source": [
    "## 8. Funzioni di training\n",
    "\n",
    "Le seguenti funzioni sono identiche allo script `train_rewts_llm_rl.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "functions"
   },
   "outputs": [],
   "source": [
    "def load_data(ticker, config):\n",
    "    \"\"\"Carica dati preprocessati\"\"\"\n",
    "    market_df = pd.read_csv(f\"data/processed/{ticker}_full_data.csv\", index_col=0, parse_dates=True)\n",
    "    news_df = pd.read_csv(f\"data/processed/{ticker}_news.csv\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    return market_df, news_df\n",
    "\n",
    "def precompute_llm_strategies(ticker, market_df, news_df, config):\n",
    "    \"\"\"Pre-computa le strategie LLM per tutto il periodo\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Pre-computing LLM Strategies for {ticker}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    strategist = StrategistAgent(config['llm'])\n",
    "    analyst = AnalystAgent(config['llm'])\n",
    "    \n",
    "    strategies = []\n",
    "    \n",
    "    # Genera strategie mensili (ogni 20 trading days)\n",
    "    strategy_frequency = config.get('strategy_frequency', 20)\n",
    "    num_strategies = len(market_df) // strategy_frequency\n",
    "    \n",
    "    for i in tqdm(range(num_strategies), desc=\"Generating strategies\"):\n",
    "        start_idx = i * strategy_frequency\n",
    "        end_idx = min((i + 1) * strategy_frequency, len(market_df))\n",
    "        \n",
    "        # Dati per questa strategia\n",
    "        period_data = market_df.iloc[start_idx:end_idx]\n",
    "        period_news = news_df[\n",
    "            (news_df.index >= period_data.index[0]) &\n",
    "            (news_df.index <= period_data.index[-1])\n",
    "        ]\n",
    "        \n",
    "        # Processa news con Analyst Agent\n",
    "        news_signals = analyst.process_news(period_news.to_dict('records'))\n",
    "        \n",
    "        # Prepara input per Strategist\n",
    "        market_data = {\n",
    "            'timestamp': str(period_data.index[-1]),\n",
    "            'Close': float(period_data['Close'].iloc[-1]),\n",
    "            'Volume': float(period_data['Volume'].iloc[-1]),\n",
    "            'Weekly_Returns': period_data['Close'].pct_change().tail(20).tolist(),\n",
    "            'HV_Close': float(period_data['HV_Close'].iloc[-1]) if 'HV_Close' in period_data else 0.0,\n",
    "            'IV_Close': float(period_data.get('IV_Close', pd.Series([0])).iloc[-1]) if 'IV_Close' in period_data else 0.0,\n",
    "            'Beta': 1.0,\n",
    "            'Classification': 'Growth'\n",
    "        }\n",
    "        \n",
    "        fundamentals = {\n",
    "            'current_ratio': float(period_data.get('Current_Ratio', pd.Series([1.5])).iloc[-1]) if 'Current_Ratio' in period_data else 1.5,\n",
    "            'debt_to_equity': float(period_data.get('Debt_to_Equity', pd.Series([0.5])).iloc[-1]) if 'Debt_to_Equity' in period_data else 0.5,\n",
    "            'pe_ratio': float(period_data.get('PE_Ratio', pd.Series([20])).iloc[-1]) if 'PE_Ratio' in period_data else 20.0,\n",
    "            'gross_margin': float(period_data.get('Gross_Margin', pd.Series([0.4])).iloc[-1]) if 'Gross_Margin' in period_data else 0.4,\n",
    "            'operating_margin': float(period_data.get('Operating_Margin', pd.Series([0.2])).iloc[-1]) if 'Operating_Margin' in period_data else 0.2,\n",
    "            'eps_yoy': 0.1,\n",
    "            'net_income_yoy': 0.1\n",
    "        }\n",
    "        \n",
    "        analytics = {\n",
    "            'ma_20': float(period_data['SMA_20'].iloc[-1]),\n",
    "            'ma_50': float(period_data['SMA_50'].iloc[-1]),\n",
    "            'ma_200': float(period_data['SMA_200'].iloc[-1]),\n",
    "            'ma_20_slope': float(period_data['SMA_20_Slope'].iloc[-1]),\n",
    "            'ma_50_slope': float(period_data['SMA_50_Slope'].iloc[-1]),\n",
    "            'rsi': float(period_data['RSI'].iloc[-1]),\n",
    "            'macd': float(period_data['MACD'].iloc[-1]),\n",
    "            'macd_signal': float(period_data['MACD_Signal'].iloc[-1]),\n",
    "            'atr': float(period_data['ATR'].iloc[-1])\n",
    "        }\n",
    "        \n",
    "        macro_data = {\n",
    "            'SPX_Close': float(period_data['SPX_Close'].iloc[-1]) if 'SPX_Close' in period_data else 0.0,\n",
    "            'SPX_Slope': float(period_data['SPX_Close'].diff().iloc[-1]) if 'SPX_Close' in period_data else 0.0,\n",
    "            'VIX_Close': float(period_data['VIX_Close'].iloc[-1]) if 'VIX_Close' in period_data else 0.0,\n",
    "            'VIX_Slope': float(period_data['VIX_Close'].diff().iloc[-1]) if 'VIX_Close' in period_data else 0.0,\n",
    "            'GDP_QoQ': 0.0,\n",
    "            'PMI': 50.0,\n",
    "            'PPI_YoY': 0.0,\n",
    "            'Treasury_YoY': 0.0\n",
    "        }\n",
    "        \n",
    "        # Genera strategia\n",
    "        last_strategy = strategies[-1] if strategies else None\n",
    "        \n",
    "        strategy = strategist.generate_strategy(\n",
    "            market_data=market_data,\n",
    "            fundamentals=fundamentals,\n",
    "            analytics=analytics,\n",
    "            macro_data=macro_data,\n",
    "            news_signals=news_signals,\n",
    "            last_strategy=last_strategy\n",
    "        )\n",
    "        \n",
    "        strategies.append(strategy)\n",
    "    \n",
    "    print(f\"‚úì Generated {len(strategies)} strategies\")\n",
    "    \n",
    "    # Salva strategies\n",
    "    os.makedirs('data/llm_strategies', exist_ok=True)\n",
    "    with open(f\"data/llm_strategies/{ticker}_strategies.pkl\", 'wb') as f:\n",
    "        pickle.dump(strategies, f)\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "def train_rewts_ensemble(ticker, market_df, strategies, config):\n",
    "    \"\"\"Addestra ReWTSE ensemble di DDQN agents\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training ReWTSE Ensemble for {ticker}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Inizializza ensemble controller\n",
    "    ensemble = ReWTSEnsembleController(config['rewts'])\n",
    "    \n",
    "    # Determina numero di chunks\n",
    "    chunk_length = config['rewts']['chunk_length']\n",
    "    num_chunks = len(market_df) // chunk_length\n",
    "    \n",
    "    print(f\"Total data points: {len(market_df)}\")\n",
    "    print(f\"Chunk length: {chunk_length}\")\n",
    "    print(f\"Number of chunks: {num_chunks}\")\n",
    "    \n",
    "    # Train un DDQN per ogni chunk\n",
    "    for chunk_id in range(num_chunks):\n",
    "        start_idx = chunk_id * chunk_length\n",
    "        end_idx = min((chunk_id + 1) * chunk_length, len(market_df))\n",
    "        \n",
    "        # Estrai chunk data\n",
    "        chunk_df = market_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Strategie LLM per questo chunk\n",
    "        strategy_start_idx = start_idx // config['strategy_frequency']\n",
    "        strategy_end_idx = end_idx // config['strategy_frequency']\n",
    "        chunk_strategies = strategies[strategy_start_idx:strategy_end_idx]\n",
    "        \n",
    "        # Assicurati che ci siano strategie\n",
    "        if len(chunk_strategies) == 0:\n",
    "            print(f\"Warning: No strategies for chunk {chunk_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Crea environment per il chunk\n",
    "        env = TradingEnv(chunk_df, chunk_strategies, config['trading_env'])\n",
    "        \n",
    "        # Addestra DDQN agent\n",
    "        agent = ensemble.train_chunk_model(\n",
    "            chunk_id=chunk_id,\n",
    "            env=env,\n",
    "            num_episodes=config['rewts']['episodes_per_chunk']\n",
    "        )\n",
    "        \n",
    "        ensemble.chunk_models.append(agent)\n",
    "    \n",
    "    print(f\"\\n‚úì Ensemble training complete!\")\n",
    "    print(f\"  Total chunk models: {len(ensemble.chunk_models)}\")\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "print(\"‚úì Funzioni di training definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_training"
   },
   "source": [
    "## 9. üöÄ Esegui Training\n",
    "\n",
    "**ATTENZIONE:** Questa cella eseguir√† il training completo.\n",
    "\n",
    "### Tempi stimati (con GPU T4):\n",
    "- 1 ticker, chunk_length=500, episodes=20: ~2-4 ore\n",
    "- Pi√π ticker o parametri pi√π alti: proporzionalmente di pi√π\n",
    "\n",
    "### Durante il training:\n",
    "- Il progresso √® mostrato con barre `tqdm`\n",
    "- I modelli sono salvati automaticamente\n",
    "- Puoi monitorare l'uso GPU con: `!nvidia-smi`\n",
    "\n",
    "### In caso di disconnessione:\n",
    "- I dati su Drive sono preservati\n",
    "- I modelli salvati restano\n",
    "- Puoi modificare il codice per riprendere da un checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# Loop su tutti i ticker\n",
    "for ticker in TRAINING_CONFIG['tickers']:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# Processing {ticker}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    # Load data\n",
    "    market_df, news_df = load_data(ticker, TRAINING_CONFIG)\n",
    "    \n",
    "    # Pre-compute LLM strategies\n",
    "    strategies = precompute_llm_strategies(ticker, market_df, news_df, TRAINING_CONFIG)\n",
    "    \n",
    "    # Train ReWTSE ensemble\n",
    "    ensemble = train_rewts_ensemble(ticker, market_df, strategies, TRAINING_CONFIG)\n",
    "    \n",
    "    # Salva ensemble (crea directory se non esiste)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open(f\"models/{ticker}_rewts_ensemble.pkl\", 'wb') as f:\n",
    "        pickle.dump(ensemble, f)\n",
    "    \n",
    "    print(f\"\\n‚úì {ticker} complete!\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úì All tickers processed successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 10. Verifica modelli salvati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_models"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RIEPILOGO MODELLI SALVATI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verifica che i modelli siano stati salvati\n",
    "models_dir = 'models'\n",
    "\n",
    "if os.path.exists(models_dir):\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n",
    "    print(f\"\\n‚úì Trovati {len(model_files)} modelli salvati:\")\n",
    "    total_size = 0\n",
    "    for f in sorted(model_files):\n",
    "        file_path = os.path.join(models_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  - {f:40s} {size_mb:>8.2f} MB\")\n",
    "    print(f\"\\nTotale modelli: {total_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Nessun modello trovato\")\n",
    "\n",
    "# Verifica strategie salvate\n",
    "strategies_dir = 'data/llm_strategies'\n",
    "if os.path.exists(strategies_dir):\n",
    "    strategy_files = [f for f in os.listdir(strategies_dir) if f.endswith('.pkl')]\n",
    "    print(f\"\\n‚úì Trovate {len(strategy_files)} strategie salvate:\")\n",
    "    total_size = 0\n",
    "    for f in sorted(strategy_files):\n",
    "        file_path = os.path.join(strategies_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        total_size += size_mb\n",
    "        print(f\"  - {f:40s} {size_mb:>8.2f} MB\")\n",
    "    print(f\"\\nTotale strategie: {total_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training completato con successo!\")\n",
    "print(\"I modelli sono salvati su Google Drive e rimarranno disponibili.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 11. Download modelli (opzionale)\n",
    "\n",
    "Se vuoi scaricare i modelli sul tuo computer locale, esegui questa cella.\n",
    "\n",
    "**Nota:** I modelli restano comunque salvati su Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Crea un archivio con i modelli\n",
    "print(\"Creazione archivio...\")\n",
    "shutil.make_archive('trained_models', 'zip', 'models')\n",
    "print(\"‚úì Modelli compressi in trained_models.zip\")\n",
    "\n",
    "# Download del file\n",
    "print(\"\\nAvvio download...\")\n",
    "files.download('trained_models.zip')\n",
    "print(\"‚úì Download avviato (controlla i download del browser)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_usage"
   },
   "source": [
    "## 12. Monitora risorse GPU (opzionale)\n",
    "\n",
    "Esegui questa cella durante il training per monitorare l'uso della GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia_smi"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## üìù Note e Suggerimenti\n",
    "\n",
    "### Risorse GPU Colab\n",
    "- **Colab Free:** GPU T4 con ~12-16GB RAM, sessioni max 12h\n",
    "- **Colab Pro:** GPU pi√π potenti (A100), sessioni pi√π lunghe\n",
    "- Salva periodicamente i checkpoint per sessioni lunghe\n",
    "\n",
    "### Ottimizzazioni per Training Veloce\n",
    "- Riduci `episodes_per_chunk`: da 20 a 10-15\n",
    "- Aumenta `chunk_length`: da 500 a 750-1000\n",
    "- Aumenta `strategy_frequency`: da 20 a 30-40\n",
    "- Riduci `buffer_size`: da 10000 a 5000\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "#### Sessione disconnessa\n",
    "- I dati su Drive sono preservati\n",
    "- Puoi riprendere il training manualmente\n",
    "- Considera di salvare checkpoint pi√π frequenti\n",
    "\n",
    "#### Out of Memory (OOM)\n",
    "- Riduci `batch_size`: da 64 a 32\n",
    "- Riduci `buffer_size`: da 10000 a 5000\n",
    "- Riduci `chunk_length`: da 500 a 250\n",
    "\n",
    "#### API Key Error\n",
    "- Verifica che GEMINI_API_KEY sia configurata\n",
    "- Controlla quota API su https://ai.google.dev/\n",
    "- Verifica connessione internet\n",
    "\n",
    "#### File non trovati\n",
    "- Verifica che PROJECT_PATH sia corretto\n",
    "- Assicurati che i dati siano caricati su Drive\n",
    "- Ri-esegui la cella di mount di Drive\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Testa con 1 ticker** prima di fare training su molti\n",
    "2. **Monitora la GPU** durante il training\n",
    "3. **Salva i log** per analisi successive\n",
    "4. **Fai backup** dei modelli su Drive\n",
    "5. **Documenta** le configurazioni che funzionano meglio\n",
    "\n",
    "### Prossimi Passi\n",
    "\n",
    "Dopo il training:\n",
    "1. Valuta i modelli su dati di test\n",
    "2. Analizza le strategie generate\n",
    "3. Esegui backtesting\n",
    "4. Confronta con baseline\n",
    "5. Ottimizza iperparametri"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
