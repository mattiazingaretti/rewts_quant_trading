{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ReWTSE-LLM-RL Trading System Training\n",
    "\n",
    "Training notebook for the hybrid LLM+RL trading system.\n",
    "\n",
    "**System Architecture:**\n",
    "- LLM Agent (Google Gemini): Generates strategic trading signals\n",
    "- RL Agent (DDQN): Learns tactical execution policies\n",
    "- ReWTSE Ensemble: Weighted ensemble with QP optimization\n",
    "\n",
    "**Usage:**\n",
    "- **Local**: Navigate to project directory and run `jupyter notebook` or use VS Code\n",
    "- **Google Colab**: Upload to Colab, mount Drive, and run all cells\n",
    "\n",
    "The notebook automatically detects if running on Colab or locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "PyTorch version: 2.9.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Detect environment (Colab or Local)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Google Drive mount (not on Colab)\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (only on Colab)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted!\")\n",
    "else:\n",
    "    print(\"Skipping Google Drive mount (not on Colab)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_project"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup project path based on environment\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colab Setup:\")\n",
    "    print(\"Choose one option:\\n\")\n",
    "    \n",
    "    # Option 1: Clone from GitHub\n",
    "    print(\"Option 1: Clone from GitHub (Recommended)\")\n",
    "    print(\"  Uncomment the following lines:\")\n",
    "    !git clone https://github.com/mattiazingaretti/rewts_quant_trading.git\n",
    "    %cd rewts_quant_trading\n",
    "    \n",
    "    # Option 2: Use files from Google Drive\n",
    "    print(\"Option 2: Use files from Google Drive\")\n",
    "    print(\"  Uncomment and modify the path to your project folder:\")\n",
    "    # project_path = '/content/drive/MyDrive/rewts_quant_trading'\n",
    "    # os.chdir(project_path)\n",
    "    \n",
    "    # Option 3: Upload manually\n",
    "    print(\"\\nOption 3: Upload project files manually\")\n",
    "    print(\"  Use the folder icon on the left sidebar to upload files\\n\")\n",
    "else:\n",
    "    print(\"Local Setup:\")\n",
    "    print(\"Make sure you're in the project root directory\\n\")\n",
    "    # Navigate to project root (assuming notebook is in notebooks/)\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "        print(\"Changed to project root directory\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"\\nDirectory contents:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing dependencies...\n",
      "\n",
      "Local environment detected.\n",
      "Using existing virtual environment or system packages.\n",
      "\n",
      "If you get import errors, install dependencies with:\n",
      "  pip install -r requirements.txt\n",
      "\n",
      "Or install individually:\n",
      "  pip install google-generativeai torch gym stable-baselines3\n",
      "  pip install pandas numpy matplotlib seaborn yfinance cvxopt tqdm pyyaml\n",
      "\n",
      "✓ Dependencies check complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "print(\"Checking and installing dependencies...\\n\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing all dependencies on Colab...\")\n",
    "    !pip install -q google-generativeai>=0.3.0\n",
    "    !pip install -q torch>=2.0.0 torchvision>=0.15.0\n",
    "    !pip install -q gym>=0.26.0\n",
    "    !pip install -q stable-baselines3>=2.0.0\n",
    "    !pip install -q pandas>=1.5.0 numpy>=1.23.0\n",
    "    !pip install -q matplotlib>=3.6.0 seaborn>=0.12.0\n",
    "    !pip install -q yfinance>=0.2.0\n",
    "    !pip install -q cvxopt>=1.3.0\n",
    "    !pip install -q tqdm>=4.64.0\n",
    "    !pip install -q pyyaml>=6.0\n",
    "else:\n",
    "    print(\"Local environment detected.\")\n",
    "    print(\"Using existing virtual environment or system packages.\")\n",
    "    print(\"\\nIf you get import errors, install dependencies with:\")\n",
    "    print(\"  pip install -r requirements.txt\")\n",
    "    print(\"\\nOr install individually:\")\n",
    "    print(\"  pip install google-generativeai torch gym stable-baselines3\")\n",
    "    print(\"  pip install pandas numpy matplotlib seaborn yfinance cvxopt tqdm pyyaml\")\n",
    "\n",
    "print(\"\\n✓ Dependencies check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_setup"
   },
   "outputs": [],
   "source": "# API Configuration\nimport os\nfrom getpass import getpass\n\n# Set your Gemini API key\n# Get your API key from: https://makersuite.google.com/app/apikey\n\n# Check if already set in environment\nif os.getenv('GEMINI_API_KEY'):\n    print(\"Using GEMINI_API_KEY from environment variables\")\n    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\nelse:\n    print(\"Enter your Gemini API Key (input will be hidden)\")\n    GEMINI_API_KEY = getpass('Gemini API Key: ')\n    os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n\n# Google AI Studio Project ID (for paid tier)\n# If you have a paid plan, set your project ID here to avoid free tier rate limits\nGOOGLE_PROJECT_ID = 'gen-lang-client-0177583380'  # Your project ID\n\nprint(\"\\n✓ API key configured!\")\nif GOOGLE_PROJECT_ID:\n    print(f\"✓ Using Google AI Studio project: {GOOGLE_PROJECT_ID}\")\n    print(\"  This will use your paid tier quota instead of free tier limits\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Monitoring API Usage\n\nWith your paid tier project configured, you can monitor API usage at:\n- **Google AI Studio**: https://aistudio.google.com/app/apikey\n- **Cloud Console**: https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas?project=gen-lang-client-0177583380\n\nYour project ID `gen-lang-client-0177583380` will be used for all API calls, ensuring you use your paid tier quota instead of free tier limits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": "# Training Configuration\nconfig = {\n    'tickers': ['AAPL'],  # Add more tickers as needed\n    \n    # LLM Configuration (Google Gemini)\n    'llm': {\n        'llm_model': 'gemini-2.0-flash-exp',  # Or 'gemini-pro' for better quality\n        'temperature': 0.0,  # Deterministic for reproducibility\n        'seed': 49,\n        'gemini_api_key': os.getenv('GEMINI_API_KEY'),\n        'project_id': GOOGLE_PROJECT_ID  # Use paid tier project\n    },\n    \n    # ReWTSE Ensemble Configuration\n    'rewts': {\n        'chunk_length': 500,  # Number of trading days per chunk\n        'lookback_length': 100,  # Lookback period for weight optimization\n        'forecast_horizon': 1,  # Steps ahead to forecast\n        'episodes_per_chunk': 50,  # Training episodes per chunk (increase for better results)\n        'gamma': 0.99,  # Discount factor\n        'epsilon_start': 1.0,  # Initial exploration rate\n        'epsilon_min': 0.01,  # Minimum exploration rate\n        'epsilon_decay': 0.995,  # Exploration decay rate\n        'learning_rate': 1e-3,  # Adam learning rate\n        'batch_size': 64,  # Mini-batch size\n        'buffer_size': 10000,  # Replay buffer size\n        'target_update_freq': 10,  # Target network update frequency\n        'hidden_dims': [128, 64]  # Neural network architecture\n    },\n    \n    # Trading Environment Configuration\n    'trading_env': {\n        'initial_balance': 10000,  # Starting capital\n        'transaction_cost': 0.001,  # 0.1% transaction cost\n        'max_position': 1.0  # Maximum position size (1.0 = 100% of capital)\n    },\n    \n    # Strategy generation frequency (days)\n    'strategy_frequency': 20  # Generate new strategy every 20 trading days (~monthly)\n}\n\nprint(\"Configuration:\")\nimport json\nprint(json.dumps({k: v for k, v in config.items() if k != 'llm'}, indent=2))\nprint(f\"\\nLLM Model: {config['llm']['llm_model']}\")\nprint(f\"LLM Project: {config['llm']['project_id']}\")\nprint(f\"Tickers: {config['tickers']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking data for AAPL:\n",
      "  Market data: ✓ (2215 rows, 2012-03-14 00:00:00-04:00 to 2020-12-30 00:00:00-05:00)\n",
      "  News data: ✓ (114 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/0sng9dls53x13h5ffn2pfkgr0000gp/T/ipykernel_78610/3847161882.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  news_df = pd.read_csv(news_data_path, index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "# Check if data files exist\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for ticker in config['tickers']:\n",
    "    market_data_path = f\"data/processed/{ticker}_full_data.csv\"\n",
    "    news_data_path = f\"data/processed/{ticker}_news.csv\"\n",
    "    \n",
    "    print(f\"\\nChecking data for {ticker}:\")\n",
    "    \n",
    "    if os.path.exists(market_data_path):\n",
    "        df = pd.read_csv(market_data_path, index_col=0, parse_dates=True)\n",
    "        print(f\"  Market data: ✓ ({len(df)} rows, {df.index.min()} to {df.index.max()})\")\n",
    "    else:\n",
    "        print(f\"  Market data: ✗ NOT FOUND at {market_data_path}\")\n",
    "    \n",
    "    if os.path.exists(news_data_path):\n",
    "        news_df = pd.read_csv(news_data_path, index_col=0, parse_dates=True)\n",
    "        print(f\"  News data: ✓ ({len(news_df)} rows)\")\n",
    "    else:\n",
    "        print(f\"  News data: ✗ NOT FOUND at {news_data_path}\")\n",
    "\n",
    "# If data is missing, you need to run the data preprocessing scripts first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if not present using the existing DataDownloader class\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts to path to import DataDownloader\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from scripts.download_data import DataDownloader\n",
    "\n",
    "# Check if data needs to be downloaded\n",
    "data_missing = False\n",
    "for ticker in config['tickers']:\n",
    "    market_data_path = f\"data/processed/{ticker}_full_data.csv\"\n",
    "    news_data_path = f\"data/processed/{ticker}_news.csv\"\n",
    "    \n",
    "    if not os.path.exists(market_data_path) or not os.path.exists(news_data_path):\n",
    "        data_missing = True\n",
    "        break\n",
    "\n",
    "if data_missing:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Downloading missing data...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Configure downloader with same parameters as training config\n",
    "    download_config = {\n",
    "        'tickers': config['tickers'],\n",
    "        'start_date': '2012-01-01',\n",
    "        'end_date': '2020-12-31',\n",
    "    }\n",
    "    \n",
    "    downloader = DataDownloader(download_config)\n",
    "    datasets = downloader.prepare_full_dataset()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ Data download complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ All data already exists\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 4. Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "import_modules"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.llm_agents.strategist_agent import StrategistAgent\n",
    "from src.llm_agents.analyst_agent import AnalystAgent\n",
    "from src.rl_agents.trading_env import TradingEnv\n",
    "from src.hybrid_model.ensemble_controller import ReWTSEnsembleController\n",
    "from src.utils.data_utils import load_market_data, load_news_data, filter_news_by_period\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "### 5.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for AAPL...\n",
      "\n",
      "Market data shape: (2215, 29)\n",
      "News data shape: (114, 4)\n",
      "\n",
      "Market data index type: <class 'pandas.core.indexes.base.Index'>\n",
      "News data index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "\n",
      "Market data columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'HV_Close', 'SPX_Close', 'VIX_Close', 'SMA_20', 'SMA_50', 'SMA_100', 'SMA_200', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'ATR', 'SMA_20_Slope', 'SMA_50_Slope', 'SMA_100_Slope', 'SMA_200_Slope', 'PE_Ratio', 'Debt_to_Equity', 'Current_Ratio', 'ROE', 'Gross_Margin', 'Operating_Margin']\n",
      "News data columns: ['Unnamed: 0', 'headline', 'summary', 'source']\n",
      "\n",
      "Market date range: 2012-03-14 00:00:00-04:00 to 2020-12-30 00:00:00-05:00\n",
      "News date range: 2012-01-03 05:00:00 to 2020-12-24 05:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load data for the first ticker\n",
    "ticker = config['tickers'][0]\n",
    "print(f\"Loading data for {ticker}...\")\n",
    "\n",
    "# Use utility functions for robust data loading\n",
    "market_df = load_market_data(ticker)\n",
    "news_df = load_news_data(ticker)\n",
    "\n",
    "print(f\"\\nMarket data shape: {market_df.shape}\")\n",
    "print(f\"News data shape: {news_df.shape}\")\n",
    "print(f\"\\nMarket data index type: {type(market_df.index)}\")\n",
    "print(f\"News data index type: {type(news_df.index)}\")\n",
    "print(f\"\\nMarket data columns: {list(market_df.columns)}\")\n",
    "print(f\"News data columns: {list(news_df.columns)}\")\n",
    "print(f\"\\nMarket date range: {market_df.index.min()} to {market_df.index.max()}\")\n",
    "print(f\"News date range: {news_df.index.min()} to {news_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "### 5.2 Pre-compute LLM Strategies\n",
    "\n",
    "This step generates strategic signals using the Gemini LLM. It may take some time depending on the data size and API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "precompute_strategies"
   },
   "outputs": [],
   "source": "import time\nimport re\n\ndef precompute_llm_strategies(ticker, market_df, news_df, config, resume=True):\n    \"\"\"\n    Pre-compute LLM strategies for the entire period with automatic retry and progress saving\n    \n    Args:\n        ticker: Stock ticker symbol\n        market_df: Market data DataFrame\n        news_df: News data DataFrame\n        config: Configuration dictionary\n        resume: If True, resume from saved progress\n    \"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Pre-computing LLM Strategies for {ticker}\")\n    print(f\"{'='*60}\")\n    \n    # Setup checkpoint directory\n    checkpoint_dir = 'data/llm_strategies/checkpoints'\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint_path = f\"{checkpoint_dir}/{ticker}_strategies_checkpoint.pkl\"\n    \n    # Try to resume from checkpoint\n    strategies = []\n    start_idx = 0\n    \n    if resume and os.path.exists(checkpoint_path):\n        try:\n            with open(checkpoint_path, 'rb') as f:\n                checkpoint_data = pickle.load(f)\n                strategies = checkpoint_data['strategies']\n                start_idx = checkpoint_data['next_idx']\n            print(f\"✓ Resuming from checkpoint: {len(strategies)} strategies already generated\")\n            print(f\"  Starting from strategy #{start_idx}\")\n        except Exception as e:\n            print(f\"Warning: Could not load checkpoint: {e}\")\n            print(\"  Starting from scratch\")\n    \n    # Initialize agents\n    strategist = StrategistAgent(config['llm'])\n    analyst = AnalystAgent(config['llm'])\n    \n    # Generate strategies at specified frequency\n    strategy_frequency = config.get('strategy_frequency', 20)\n    num_strategies = len(market_df) // strategy_frequency\n    \n    print(f\"\\nTotal data points: {len(market_df)}\")\n    print(f\"Strategy frequency: every {strategy_frequency} days\")\n    print(f\"Total strategies to generate: {num_strategies}\")\n    print(f\"Remaining: {num_strategies - start_idx}\")\n    \n    # API rate limiting parameters\n    # Adjust based on tier: paid tier has much higher limits\n    project_id = config['llm'].get('project_id')\n    if project_id:\n        # Paid tier: ~2000 requests/minute for gemini-2.0-flash\n        requests_per_minute = 60  # Conservative limit for paid tier\n        print(f\"\\n✓ Using paid tier project: {project_id}\")\n    else:\n        # Free tier: 10 requests/minute\n        requests_per_minute = 10\n        print(\"\\n⚠ Using free tier (no project ID)\")\n    \n    delay_between_requests = 60.0 / requests_per_minute + 0.2  # Add 0.2s buffer\n    \n    print(f\"\\nAPI Rate Limiting:\")\n    print(f\"  Requests per minute: {requests_per_minute}\")\n    print(f\"  Delay between requests: {delay_between_requests:.1f}s\")\n    print(f\"  Estimated time: {(num_strategies - start_idx) * delay_between_requests / 60:.1f} minutes\")\n    \n    for i in tqdm(range(start_idx, num_strategies), desc=\"Generating LLM strategies\", initial=start_idx, total=num_strategies):\n        strategy_start_idx = i * strategy_frequency\n        strategy_end_idx = min((i + 1) * strategy_frequency, len(market_df))\n        \n        # Data for this strategy period\n        period_data = market_df.iloc[strategy_start_idx:strategy_end_idx]\n        \n        # Filter news for this period using utility function\n        period_news = filter_news_by_period(\n            news_df,\n            period_data.index[0],\n            period_data.index[-1]\n        )\n        \n        # Process news with Analyst Agent with retry logic\n        max_retries = 3\n        retry_count = 0\n        news_signals = None\n        \n        while retry_count < max_retries and news_signals is None:\n            try:\n                if len(period_news) > 0:\n                    news_signals = analyst.process_news(period_news.to_dict('records'))\n                else:\n                    # No news available for this period\n                    news_signals = {\n                        'sentiment': 'neutral',\n                        'confidence': 0.5,\n                        'key_topics': []\n                    }\n                \n                # Add delay after successful API call\n                time.sleep(delay_between_requests)\n                \n            except Exception as e:\n                error_msg = str(e)\n                \n                # Check if it's a rate limit error\n                if '429' in error_msg or 'quota' in error_msg.lower():\n                    # Extract wait time from error message if available\n                    wait_time = 15 if not project_id else 5  # Shorter wait for paid tier\n                    match = re.search(r'retry in (\\d+\\.?\\d*)s', error_msg)\n                    if match:\n                        wait_time = float(match.group(1)) + 1  # Add 1s buffer\n                    \n                    print(f\"\\n⚠ Rate limit reached. Waiting {wait_time:.0f}s...\")\n                    time.sleep(wait_time)\n                    retry_count += 1\n                else:\n                    # Other error, use neutral sentiment\n                    print(f\"\\n⚠ Error processing news: {error_msg}\")\n                    news_signals = {\n                        'sentiment': 'neutral',\n                        'confidence': 0.5,\n                        'key_topics': []\n                    }\n                    break\n        \n        # If still failed, use neutral\n        if news_signals is None:\n            news_signals = {\n                'sentiment': 'neutral',\n                'confidence': 0.5,\n                'key_topics': []\n            }\n        \n        # Prepare input for Strategist\n        market_data = {\n            'timestamp': str(period_data.index[-1]),\n            'Close': float(period_data['Close'].iloc[-1]),\n            'Volume': float(period_data['Volume'].iloc[-1]),\n            'Weekly_Returns': period_data['Close'].pct_change().tail(20).tolist(),\n            'HV_Close': float(period_data.get('HV_Close', pd.Series([0])).iloc[-1]),\n            'IV_Close': float(period_data.get('IV_Close', pd.Series([0])).iloc[-1]),\n            'Beta': 1.0,\n            'Classification': 'Growth'\n        }\n        \n        fundamentals = {\n            'current_ratio': float(period_data.get('Current_Ratio', pd.Series([1.5])).iloc[-1]),\n            'debt_to_equity': float(period_data.get('Debt_to_Equity', pd.Series([0.5])).iloc[-1]),\n            'pe_ratio': float(period_data.get('PE_Ratio', pd.Series([20])).iloc[-1]),\n            'gross_margin': float(period_data.get('Gross_Margin', pd.Series([0.4])).iloc[-1]),\n            'operating_margin': float(period_data.get('Operating_Margin', pd.Series([0.2])).iloc[-1]),\n            'eps_yoy': 0.1,\n            'net_income_yoy': 0.1\n        }\n        \n        analytics = {\n            'ma_20': float(period_data['SMA_20'].iloc[-1]),\n            'ma_50': float(period_data['SMA_50'].iloc[-1]),\n            'ma_200': float(period_data['SMA_200'].iloc[-1]),\n            'ma_20_slope': float(period_data['SMA_20_Slope'].iloc[-1]),\n            'ma_50_slope': float(period_data['SMA_50_Slope'].iloc[-1]),\n            'rsi': float(period_data['RSI'].iloc[-1]),\n            'macd': float(period_data['MACD'].iloc[-1]),\n            'macd_signal': float(period_data['MACD_Signal'].iloc[-1]),\n            'atr': float(period_data['ATR'].iloc[-1])\n        }\n        \n        macro_data = {\n            'SPX_Close': float(period_data.get('SPX_Close', pd.Series([0])).iloc[-1]),\n            'SPX_Slope': float(period_data['SPX_Close'].diff().iloc[-1]) if 'SPX_Close' in period_data else 0.0,\n            'VIX_Close': float(period_data.get('VIX_Close', pd.Series([0])).iloc[-1]),\n            'VIX_Slope': float(period_data['VIX_Close'].diff().iloc[-1]) if 'VIX_Close' in period_data else 0.0,\n            'GDP_QoQ': 0.0,\n            'PMI': 50.0,\n            'PPI_YoY': 0.0,\n            'Treasury_YoY': 0.0\n        }\n        \n        # Generate strategy with retry logic\n        strategy = None\n        retry_count = 0\n        \n        while retry_count < max_retries and strategy is None:\n            try:\n                last_strategy = strategies[-1] if strategies else None\n                \n                strategy = strategist.generate_strategy(\n                    market_data=market_data,\n                    fundamentals=fundamentals,\n                    analytics=analytics,\n                    macro_data=macro_data,\n                    news_signals=news_signals,\n                    last_strategy=last_strategy\n                )\n                \n                # Add delay after successful API call\n                time.sleep(delay_between_requests)\n                \n            except Exception as e:\n                error_msg = str(e)\n                \n                # Check if it's a rate limit error\n                if '429' in error_msg or 'quota' in error_msg.lower():\n                    wait_time = 15 if not project_id else 5\n                    match = re.search(r'retry in (\\d+\\.?\\d*)s', error_msg)\n                    if match:\n                        wait_time = float(match.group(1)) + 1\n                    \n                    print(f\"\\n⚠ Rate limit reached. Waiting {wait_time:.0f}s...\")\n                    time.sleep(wait_time)\n                    retry_count += 1\n                else:\n                    print(f\"\\n⚠ Error generating strategy: {error_msg}\")\n                    raise  # Re-raise non-rate-limit errors\n        \n        if strategy is None:\n            raise Exception(\"Failed to generate strategy after multiple retries\")\n        \n        strategies.append(strategy)\n        \n        # Save checkpoint every 10 strategies\n        if (i + 1) % 10 == 0 or (i + 1) == num_strategies:\n            checkpoint_data = {\n                'strategies': strategies,\n                'next_idx': i + 1,\n                'ticker': ticker,\n                'timestamp': time.time()\n            }\n            with open(checkpoint_path, 'wb') as f:\n                pickle.dump(checkpoint_data, f)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"✓ Generated {len(strategies)} strategies\")\n    print(f\"{'='*60}\")\n    \n    # Save final strategies\n    os.makedirs('data/llm_strategies', exist_ok=True)\n    final_path = f\"data/llm_strategies/{ticker}_strategies.pkl\"\n    with open(final_path, 'wb') as f:\n        pickle.dump(strategies, f)\n    \n    print(f\"✓ Strategies saved to {final_path}\")\n    \n    # Clean up checkpoint\n    if os.path.exists(checkpoint_path):\n        os.remove(checkpoint_path)\n        print(f\"✓ Checkpoint cleaned up\")\n    \n    return strategies\n\n# Generate strategies with auto-resume\nstrategies = precompute_llm_strategies(ticker, market_df, news_df, config, resume=True)\n\n# Display sample strategies\nprint(f\"\\nSample strategies:\")\nfor i, strategy in enumerate(strategies[:3]):\n    print(f\"\\nStrategy {i+1}:\")\n    print(f\"  Direction: {strategy.direction}\")\n    print(f\"  Strength: {strategy.strength:.2f}\")\n    print(f\"  Signal (τ): {(2*strategy.direction-1) * strategy.strength:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "### 5.3 Train ReWTSE Ensemble\n",
    "\n",
    "This is the main training loop. It trains multiple DDQN agents on different time chunks and creates an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ensemble"
   },
   "outputs": [],
   "source": [
    "def train_rewts_ensemble(ticker, market_df, strategies, config):\n",
    "    \"\"\"Train ReWTSE ensemble of DDQN agents\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training ReWTSE Ensemble for {ticker}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize ensemble controller\n",
    "    ensemble = ReWTSEnsembleController(config['rewts'])\n",
    "    \n",
    "    # Determine number of chunks\n",
    "    chunk_length = config['rewts']['chunk_length']\n",
    "    num_chunks = len(market_df) // chunk_length\n",
    "    \n",
    "    print(f\"Total data points: {len(market_df)}\")\n",
    "    print(f\"Chunk length: {chunk_length}\")\n",
    "    print(f\"Number of chunks: {num_chunks}\")\n",
    "    print(f\"Episodes per chunk: {config['rewts']['episodes_per_chunk']}\")\n",
    "    \n",
    "    # Train a DDQN for each chunk\n",
    "    for chunk_id in range(num_chunks):\n",
    "        start_idx = chunk_id * chunk_length\n",
    "        end_idx = min((chunk_id + 1) * chunk_length, len(market_df))\n",
    "        \n",
    "        # Extract chunk data\n",
    "        chunk_df = market_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # LLM strategies for this chunk\n",
    "        strategy_start_idx = start_idx // config['strategy_frequency']\n",
    "        strategy_end_idx = end_idx // config['strategy_frequency']\n",
    "        chunk_strategies = strategies[strategy_start_idx:strategy_end_idx]\n",
    "        \n",
    "        # Ensure we have strategies\n",
    "        if len(chunk_strategies) == 0:\n",
    "            print(f\"Warning: No strategies for chunk {chunk_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nChunk {chunk_id}: {len(chunk_df)} days, {len(chunk_strategies)} strategies\")\n",
    "        \n",
    "        # Create environment for the chunk\n",
    "        env = TradingEnv(chunk_df, chunk_strategies, config['trading_env'])\n",
    "        \n",
    "        # Train DDQN agent\n",
    "        agent = ensemble.train_chunk_model(\n",
    "            chunk_id=chunk_id,\n",
    "            env=env,\n",
    "            num_episodes=config['rewts']['episodes_per_chunk']\n",
    "        )\n",
    "        \n",
    "        ensemble.chunk_models.append(agent)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Ensemble training complete!\")\n",
    "    print(f\"  Total chunk models: {len(ensemble.chunk_models)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble = train_rewts_ensemble(ticker, market_df, strategies, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "### 5.4 Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save the ensemble model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = f\"models/{ticker}_rewts_ensemble.pkl\"\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(ensemble, f)\n",
    "\n",
    "print(f\"✓ Ensemble model saved to {model_path}\")\n",
    "\n",
    "# Save to Google Drive (only on Colab)\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        drive_models_path = '/content/drive/MyDrive/Papers/models'\n",
    "        os.makedirs(drive_models_path, exist_ok=True)\n",
    "        \n",
    "        drive_model_path = f\"{drive_models_path}/{ticker}_rewts_ensemble.pkl\"\n",
    "        with open(drive_model_path, 'wb') as f:\n",
    "            pickle.dump(ensemble, f)\n",
    "        \n",
    "        print(f\"✓ Ensemble model also saved to Google Drive: {drive_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save to Drive: {e}\")\n",
    "else:\n",
    "    print(f\"✓ Model saved locally in the project directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Quick evaluation on training data\n",
    "print(\"Evaluating ensemble on training data...\\n\")\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = TradingEnv(market_df, strategies, config['trading_env'])\n",
    "\n",
    "# Initialize weights (uniform for now)\n",
    "if len(ensemble.chunk_models) > 0:\n",
    "    ensemble.current_weights = np.ones(len(ensemble.chunk_models)) / len(ensemble.chunk_models)\n",
    "\n",
    "state = eval_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "actions_taken = []\n",
    "\n",
    "while not done:\n",
    "    # Get ensemble action\n",
    "    action, _ = ensemble.predict_ensemble(state)\n",
    "    actions_taken.append(action)\n",
    "    \n",
    "    # Execute action\n",
    "    state, reward, done, _ = eval_env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "# Results\n",
    "final_portfolio_value = eval_env.portfolio_value\n",
    "initial_balance = config['trading_env']['initial_balance']\n",
    "total_return = (final_portfolio_value - initial_balance) / initial_balance * 100\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training Data Evaluation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Initial Balance: ${initial_balance:,.2f}\")\n",
    "print(f\"Final Portfolio Value: ${final_portfolio_value:,.2f}\")\n",
    "print(f\"Total Return: {total_return:.2f}%\")\n",
    "print(f\"Total Reward: {total_reward:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Action distribution\n",
    "action_names = ['SHORT', 'HOLD', 'LONG']\n",
    "action_counts = np.bincount(actions_taken, minlength=3)\n",
    "print(f\"\\nAction Distribution:\")\n",
    "for i, name in enumerate(action_names):\n",
    "    pct = action_counts[i] / len(actions_taken) * 100\n",
    "    print(f\"  {name}: {action_counts[i]} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Plot portfolio value over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eval_env.portfolio_history, label='Portfolio Value', linewidth=2)\n",
    "plt.axhline(y=initial_balance, color='r', linestyle='--', label='Initial Balance')\n",
    "plt.title(f'{ticker} - Portfolio Value Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Trading Steps')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "returns = np.diff(eval_env.portfolio_history) / eval_env.portfolio_history[:-1]\n",
    "plt.hist(returns, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.title('Return Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
    "max_drawdown = np.min(returns) if len(returns) > 0 else 0\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Sharpe Ratio (annualized): {sharpe_ratio:.2f}\")\n",
    "print(f\"  Max Drawdown: {max_drawdown*100:.2f}%\")\n",
    "print(f\"  Volatility (annualized): {np.std(returns)*np.sqrt(252)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "After training, you can:\n",
    "\n",
    "1. **Backtest**: Run the backtesting script to evaluate on test data\n",
    "2. **Add more tickers**: Modify `config['tickers']` to train on multiple stocks\n",
    "3. **Tune hyperparameters**: Adjust learning rate, episodes, chunk length, etc.\n",
    "4. **Paper trading**: Use the Alpaca API to test on live data without real money\n",
    "5. **Analyze ensemble weights**: Examine how the QP optimization weights different chunks\n",
    "\n",
    "**Important Notes:**\n",
    "- Training may take several hours depending on data size and configuration\n",
    "- For production use, train on more episodes and larger datasets\n",
    "- Always validate on out-of-sample test data before real trading\n",
    "- Consider implementing proper train/validation/test splits\n",
    "- Monitor API usage and costs for Gemini calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_checkpoint"
   },
   "outputs": [],
   "source": [
    "# Save training checkpoint with metadata\n",
    "import datetime\n",
    "\n",
    "checkpoint = {\n",
    "    'ensemble': ensemble,\n",
    "    'strategies': strategies,\n",
    "    'config': config,\n",
    "    'ticker': ticker,\n",
    "    'training_date': datetime.datetime.now().isoformat(),\n",
    "    'data_period': {\n",
    "        'start': str(market_df.index.min()),\n",
    "        'end': str(market_df.index.max()),\n",
    "        'num_days': len(market_df)\n",
    "    },\n",
    "    'performance': {\n",
    "        'final_value': final_portfolio_value,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe_ratio\n",
    "    }\n",
    "}\n",
    "\n",
    "checkpoint_path = f\"models/{ticker}_checkpoint_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "with open(checkpoint_path, 'wb') as f:\n",
    "    pickle.dump(checkpoint, f)\n",
    "\n",
    "print(f\"✓ Training checkpoint saved to {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_rewts_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}